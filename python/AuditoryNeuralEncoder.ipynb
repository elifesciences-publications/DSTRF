{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from hdf5storage import loadmat, savemat\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load provided sample data, which consists of a training set of 54 trials and a test set of multiple repetitions of 8 unique trials. Here, the repetitions of the test set have been already averaged into two groups of responses to odd repetitions and even repetitions. The stimuli are in form of auditory spectrograms of the presented audio and responses are preprocessed and z-scored highgamma envelopes of three sample ECoG electrodes.\n",
    "\n",
    "$X_{tr}$ and $Y_{tr}$ are the stimulus and response data for the train set, respectively, while $X_{te}$ and $Y_{te}$ represent the test set. $X_{tr}$ and $X_{te}$ consist of multiple trials, each with shape $[time \\times freq\\_bins]$. $Y_{tr}$ and $Y_{te}$ have shapes $[time \\times channels]$ and $[time \\times channels \\times repetitions]$ per trial, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('../sample_data_keshishian_etal.mat')\n",
    "\n",
    "# train set\n",
    "X_tr = data['X_tr'].flatten()\n",
    "Y_tr = data['Y_tr'].flatten()\n",
    "\n",
    "# test set\n",
    "X_te = data['X_te'].flatten()\n",
    "Y_te = data['Y_te'].flatten()\n",
    "\n",
    "# separate even and odd repetitions of test set\n",
    "Y_r0 = np.array([y[:,:,0::2].mean(-1) for y in Y_te])\n",
    "Y_r1 = np.array([y[:,:,1::2].mean(-1) for y in Y_te])\n",
    "Y_te = np.array([y.mean(-1) for y in Y_te])\n",
    "\n",
    "# stimulus frequency channels\n",
    "freq_bins = X_tr[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display sample stimulus and response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(211), plt.imshow(X_tr[0][:300].T, origin=\"ll\", aspect=2), plt.xlim([0, 300])\n",
    "plt.subplot(212), plt.plot(Y_tr[0][:300,0]), plt.xlim([0, 300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the train set into two train and validation subsets for training cross-validation. This data split can be done using jackknife, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set aside two trials for validation\n",
    "trials_vl = np.zeros(len(X_tr), dtype=bool)\n",
    "trials_vl[[1, 3]] = 1\n",
    "\n",
    "X_vl = X_tr[trials_vl]\n",
    "Y_vl = Y_tr[trials_vl]\n",
    "\n",
    "X_tr = X_tr[~trials_vl]\n",
    "Y_tr = Y_tr[~trials_vl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select target channel from list of electrodes. The new variables are 1-dimensional timecourses per trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 0\n",
    "print(f\"Fitting models for channel {channel:d}...\")\n",
    "\n",
    "Y_tr = np.array([y[:, channel] for y in Y_tr])\n",
    "Y_vl = np.array([y[:, channel] for y in Y_vl])\n",
    "Y_te = np.array([y[:, channel] for y in Y_te])\n",
    "Y_r0 = np.array([y[:, channel] for y in Y_r0])\n",
    "Y_r1 = np.array([y[:, channel] for y in Y_r1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all trials into a single numpy variable for ease of training. Trials are spaced by 500ms to avoid between-trial contamination by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.concatenate([np.pad(x, ((25, 25), (0, 0)), constant_values=0) for x in X_tr])\n",
    "Y_tr = np.concatenate([np.pad(x, ((25, 25),), constant_values=np.nan) for x in Y_tr])\n",
    "\n",
    "X_vl = np.concatenate([np.pad(x, ((25, 25), (0, 0)), constant_values=0) for x in X_vl])\n",
    "Y_vl = np.concatenate([np.pad(x, ((25, 25),), constant_values=np.nan) for x in Y_vl])\n",
    "\n",
    "X_te = np.concatenate([np.pad(x, ((25, 25), (0, 0)), constant_values=0) for x in X_te])\n",
    "Y_te = np.concatenate([np.pad(x, ((25, 25),), constant_values=np.nan) for x in Y_te])\n",
    "Y_r0 = np.concatenate([np.pad(x, ((25, 25),), constant_values=np.nan) for x in Y_r0])\n",
    "Y_r1 = np.concatenate([np.pad(x, ((25, 25),), constant_values=np.nan) for x in Y_r1])\n",
    "\n",
    "print(X_tr.shape, X_vl.shape, X_te.shape)\n",
    "print(Y_tr.shape, Y_vl.shape, Y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into continous chunks for batch training if needed.<br/>Network inputs must have shape $[batch \\times time \\times freq\\_bins]$ and outputs must have shape $[batch \\times time]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use full data as single batch\n",
    "X_tr = X_tr[np.newaxis]\n",
    "Y_tr = Y_tr[np.newaxis]\n",
    "\n",
    "X_vl = X_vl[np.newaxis]\n",
    "Y_vl = Y_vl[np.newaxis]\n",
    "\n",
    "X_te = X_te[np.newaxis]\n",
    "Y_te = Y_te[np.newaxis]\n",
    "Y_r0 = Y_r0[np.newaxis]\n",
    "Y_r1 = Y_r1[np.newaxis]\n",
    "\n",
    "print(X_tr.shape, X_vl.shape, X_te.shape)\n",
    "print(Y_tr.shape, Y_vl.shape, Y_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan elements for computing loss\n",
    "def drop_nan(response, prediction):\n",
    "    mask = tf.math.is_finite(response)\n",
    "    return tf.boolean_mask(response, mask), tf.boolean_mask(prediction, mask)\n",
    "\n",
    "# loss function\n",
    "def loss_se(response, prediction):\n",
    "    \"\"\"Squared error loss.\"\"\"\n",
    "    response, prediction = drop_nan(response, prediction)\n",
    "    num = tf.reduce_mean(tf.square(response - prediction))\n",
    "    den = tf.reduce_mean(tf.square(response))\n",
    "    # den = tf.reduce_mean(tf.square(response - tf.reduce_mean(response)))\n",
    "    return num / den\n",
    "\n",
    "# correlation metric\n",
    "def metric_corr(response, prediction):\n",
    "    response, prediction = drop_nan(response, prediction)\n",
    "    response, prediction = tf.expand_dims(response, 0), tf.expand_dims(prediction, 0)\n",
    "    return tfp.stats.correlation(response, prediction, 1, 0)\n",
    "\n",
    "# noise-corrected correlation\n",
    "def fn_ncorr(prediction):\n",
    "    mask = np.isfinite(Y_te)\n",
    "    r0 = scipy.stats.pearsonr(Y_r0[mask], prediction[mask])[0]\n",
    "    r1 = scipy.stats.pearsonr(Y_r1[mask], prediction[mask])[0]\n",
    "    rr = scipy.stats.pearsonr(Y_r0[mask], Y_r1[mask])[0]\n",
    "    return (r0 + r1)/2 / np.sqrt(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = tf.keras.regularizers.l2(0.0001)\n",
    "layer_opts = dict(activation='relu', use_bias=False, kernel_regularizer=l2)\n",
    "\n",
    "layers = (\n",
    "    tf.keras.layers.InputLayer(input_shape=(None, freq_bins)),\n",
    "    tf.keras.layers.Reshape((-1, freq_bins, 1)),\n",
    "    \n",
    "    tf.keras.layers.ZeroPadding2D(((2, 0), (1, 1))),\n",
    "    tf.keras.layers.Conv2D(16, 3, **layer_opts),\n",
    "    tf.keras.layers.ZeroPadding2D(((2, 0), (1, 1))),\n",
    "    tf.keras.layers.Conv2D(16, 3, **layer_opts),\n",
    "    tf.keras.layers.ZeroPadding2D(((2, 0), (1, 1))),\n",
    "    tf.keras.layers.Conv2D(16, 3, **layer_opts),\n",
    "    \n",
    "    # tf.keras.layers.ZeroPadding2D(((1, 0), (0, 0))),\n",
    "    # tf.keras.layers.MaxPool2D((2, 2), (1, 2)),\n",
    "    \n",
    "    tf.keras.layers.ZeroPadding2D(((39, 0), (0, 0))),\n",
    "    tf.keras.layers.Conv2D(1, (40, 32), use_bias=True, kernel_regularizer=l2),\n",
    "    tf.keras.layers.Flatten()\n",
    ")\n",
    "\n",
    "blueprint = tf.keras.models.Sequential(layers)\n",
    "blueprint.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directory for saving trained models\n",
    "model_dir = f\"models/\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# path for saving model\n",
    "model_path = os.path.join(model_dir, f\"mdl-v1-{channel:03d}\")\n",
    "\n",
    "# initialize model from blueprint\n",
    "model = tf.keras.models.clone_model(blueprint)\n",
    "#optim = tf.keras.optimizers.RMSprop(1e-3, momentum=0.9)\n",
    "optim = tf.keras.optimizers.Adam(1e-3)\n",
    "model.compile(optimizer=optim, loss=loss_se, metrics=[metric_corr])\n",
    "\n",
    "# set callbacks\n",
    "callbk_early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_metric_corr', patience=500, mode='max', restore_best_weights=True)\n",
    "callbk_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_path, monitor='val_metric_corr', save_best_only=True, mode='max')\n",
    "callbacks = [callbk_early_stop, callbk_checkpoint]\n",
    "\n",
    "# fit model to data\n",
    "history = model.fit(X_tr, Y_tr, validation_data=(X_vl, Y_vl),\n",
    "                    epochs=5000, verbose=1, callbacks=callbacks)\n",
    "\n",
    "# plot performance curves\n",
    "plt.figure()\n",
    "plt.plot(history.history['metric_corr'])\n",
    "plt.plot(history.history['val_metric_corr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f\"{model_base:s}.h5\")\n",
    "pred = model(X_te)\n",
    "print(f\"\\tsplit #{jk+1}\\t{fn_ncorr(pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate dynamic STRFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dynamic STRFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
